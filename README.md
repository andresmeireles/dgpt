# DGPT - Um chat ai menos perspicaz*

###### dependendo do modelo usado ele pode ser bem mais perspicaz üòÅ

**dgpt** √© uma sistema de chat com ia onde voce pode usar diversos modelos para conversa. Utiliza php com [frankenphp](https://frankenphp.dev) como runtime, [slim framework](https://www.slimframework.com), [twig](https://twig.symfony.com), [tailwind](https://tailwindcss.com). Utiliza o protocolo [mercure](https://mercure.rocks), uma alternativa ao websocket, para mensagens instant√¢neas no chat. Como motor de ia utiliza o [ollma](https://ollama.ai).

## Setup do projeto

O projeto possui um arquivo **docker-compose** que subir√° todos os servi√ßos, a partir disso algumas configura√ß√µes tem que ser feitas, √© necess√°rio ter instalado:

* PHP na vers√£o 8.2 ou superior

Apos isso instale as depend√™ncias do projeto com `composer install`.

Depois voce deve copiar o arquivo `.env.example` e renome-lo para `.env` e preencher os campos, um exemplo de configura√ß√£o:

```
MODE=dev
OLLAMA_HOST=ollama
OLLAMA_PORT=11434
OLLAMA_LLM_PATH=/llm
```

Onde:

* **MODE** define o modo em que a aplica√ß√£o vai funcionar, mais explica√ß√µes abaixo.
* **OLLAMA_HOST** o host do servidor ollama, o padr√£o do projeto √© ollama.
* **OLLAMA_PORT** a porta do servidor, o padr√£o √© 11434.
* **OLLAMA_LLM_PATH** √© o caminho, a partir do root do projeto, onde os llm's ser√£o armazenados.

Apos isso o projeto esta pronto para ser usado no caminho `https://localhost`.

## Como utilizar o sistema

Como o projeto propriamente instalado e com um modelo instalado √© so escrever seu prompt, a velocidade e acur√°cia da resposta vai depender da sua maquina e do seu modelo. 

## Como adicionar LLM's?

Os LLM's (large language model) sao centro deste projeto. √â necess√°rio ter ao menos um LLM para que o chat funcione. Por padr√£o o projeto nao vem com nenhum LLM, por causa do seu tamanho (eles costumam ser bem grandes).

O primeiro passo para adicionar um llm √© baixar um modelo, eles podem ser encontrados no site do [huggingface](https://huggingface.co), os modelos baixados devem ser no formato `gguf`, baixe um que se ad√©que a sua maquina, um modelo de 2_7B de par√¢metros normalmente pede 2G de memoria ram um modelo de 7B necessita de 7 a 8GB e assim por diante.

Com um modelo baixado adicione esse modelo na pasta `llm` ou pasta que voce definiu no arquivo `.env` dentro da pasta crie um arquivo com o mesmo nome do modelo mas sem nenhuma extens√£o, por exemplo, baixei o modelo `openchat_3.5.Q2_K.gguf` entrao crio um arquivo `openchat_3.5.Q2_K` (sem extens√£o alguma), o conte√∫do deste arquivo deve ser:

```
FROM ./openchat_3.5.Q2_K.gguf
```

Mais op√ß√µes sobre o que por neste arquivo podem ser encontrados no site do ollama [por link aqui]

Recarrega a pagina do sistema e vera que na barra lateral haver√° uma op√ß√£o chamada `openchat_3.5.Q2_K` com uma op√ß√£o chamada `Add` aperte neste bot√£o e o modelo sera adicionado ao sistema, isso pode demora um pouco de acordo com sua maquina e do tamanho do modelo. V√°rios modelos podem ser adicionados ao sistema.

## Como utilizar o worker mode do Frankenphp

Uma das features mais legais do **frankenphp** √© o [*worker mode*](https://frankenphp.dev/docs/worker/) um modo apos o primeiro boot a aplica√ß√£o php fica em memoria, deixando-a bastante eficaz.

Primeiramente, todas as vari√°veis no arquivo `.env` devem ser passadas para o arquivo `docker-compose.yml`. A inser√ß√£o de var√°veis de ambiente via `.env` nao funciona de forma consistente.

O primeiro passo √© no arquivo `.env` mudar o `MODE` para `prod` ou `worker` no arquivo `docker-compose.yml` descomete a linha.

`#  - FRANKENPHP_CONFIG=worker public/index.php`

Depois reinicie o container do php.

Na minha percep√ß√£o o worker mode nao faz muita diferen√ßa nesta aplica√ß√£o, da sua arquitetura atual, pois ela faz chamadas a outro servidor, porem se a abordagem com a ia for feita de outro modo, usando o llama.cpp por exemplo, onde o processamento √© local, acho que o workermode traria mais benef√≠cios, acredito que houve alguma melhora de performance, mas nao tanta nesse caso de uso.  

